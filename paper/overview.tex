\section{Overview of Approach}
Several methods for solving math word problems work by mapping problem text to underlying equations (CITE kk, kush, Roy). 
The approach outlined here follows the method of (KK and Roy) in structuring underlying equations in tree form (See Figure TREE).The structure of equation trees allows for their decomposition, so that learning can take place locally. 
Each intermediate node representes some combination of quantities, the result of which is itself a quantity that may enter into further combinations. 
The text of the problem, if well understood, indicates the correct operations for recursively combining quantities. 

Our method learns the correct recursive combination of quantities.
This involves identifying both operation that combines any two quantities in the tree as well as the overall equation tree structure, which can be seen as a partial order of operations. 

\paragraph{Training}
In the training phase, we first produce a set of equation trees which yeild the correct answer, utilizing the Integer Linear Program described in (CITE KK). 
This program uses ILP to rank generate the top-k best trees according to a set of possibly soft constraints including equations complexity and type constraints.
Quantity ``types'' are the nouns associated via a dependency parse relation with a numerical value in the problem.
An example type constraint is a penalty against multiplying two quantities of the same type. 
We take the correct equations from the top 100 trees generated by this method.

We then decompose these trees into triples consisting of two quantities and an operation $op$. 
We associate each quantity with a intermediate representation of that quantity in the problem text, derived from a domain independent semantic parse. 
The details of this representation are described in Section~\ref{semantics}
We then construct a vector based on the semantics of the two quantities, which serves as positive example for the $op$ operation, and a negative sample for all other operators. 
The details of this vector representation are described in Section~\ref{vectors}

\paragraph{Inference} At inference, we consider the top k trees for an unseen problem as provided by the ILP. 
We then score all operations in all trees with the model learned in training.
The final decision is made by multiplying the scores provided for each intermediate node and taking the highest scoring tree. 
If this tree yields a correct answer for the word problem, we consider this to be a success. 
